{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9dfe9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required pip installs: numpy, pandas, matplotlib, nltk, spacy, textblob, pickle, tqdm, sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from textblob import Word\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from os import listdir\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53ef301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23624e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file structure should have the \"news\" folder containing the 3 .csv's in the same root folder as this noteboo\n",
    "\n",
    "# Load the data from the disk into a pandas dataframe\n",
    "\n",
    "dfs = []\n",
    "for x in [1, 2, 3]:\n",
    "    for chunk in pd.read_csv(f'./news/articles{x}.csv', chunksize = 100):\n",
    "        dfs.append(chunk)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d519e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to define function that converts articles to lowercase and perfom lemmatization\n",
    "\n",
    "def preprocess(row):\n",
    "    \n",
    "    row = row.lower()\n",
    "    \n",
    "    # lemmatize\n",
    "    row = ' '.join([Word(word).lemmatize() for word in row.split()])\n",
    "    \n",
    "    # remove any extra whitespace\n",
    "    row = re.sub('\\s{1,}', ' ', row)\n",
    "    \n",
    "    return ' '.join([word for word in row.split() if len(word) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "098674b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c26278c8cfd4b439db7f4748c70d9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run to apply the preprocessing to the articles if 'processed_articles.csv' does not already exist (will take a while)\n",
    "# If it already exists, skip to the cell below\n",
    "\n",
    "df_processed = df.copy(deep = True)\n",
    "\n",
    "for i in trange(df.shape[0]):\n",
    "    df_processed.loc[i, 'content'] = preprocess(df_processed.loc[i, 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7ef43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to load the preprocessed articles dataframe directly from the disk. The 'processed_articles.csv' should be\n",
    "# placed within the same root directory as this notebook file\n",
    "\n",
    "dfs = []\n",
    "for chunk in pd.read_csv('processed_articles.csv', chunksize = 100):\n",
    "    dfs.append(chunk)\n",
    "df_processed = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2007df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save processed dataframe created in the cell above\n",
    "\n",
    "df_processed.to_csv('processed_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c85eea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to remove any weird blank articles\n",
    "\n",
    "df_processed = df_processed[pd.isna(df_processed['title']) == False]\n",
    "df_processed = df_processed[pd.isna(df_processed['content']) == False]\n",
    "df = df[pd.isna(df['title']) == False]\n",
    "df = df[pd.isna(df['content']) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3dd6ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to make date formats uniform\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'],format='%Y-%m-%d')\n",
    "df_processed['date'] = pd.to_datetime(df_processed['date'],format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "421f3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to create the sklearn tfidf model if the 'model.pkl' file does not already exist\n",
    "# If it does, load it from the cell below\n",
    "\n",
    "content = df_processed['content']\n",
    "model = TfidfVectorizer(stop_words = 'english', use_idf = True, smooth_idf = True, min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ae8f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sklearn tfidf model from the disk. The file should be in the same root directory as this notebook file\n",
    "\n",
    "model = pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc3cef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to create the tf-idf matrix corresponding to the articles content dataframe  if the 'tfidf.pkl' file does not already exist\n",
    "# If it does, load the tfidf matrix from the cell below\n",
    "\n",
    "tfidf = model.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3225897",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open('tfidf.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d0b3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tf-idf matrix\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "447a4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that calculates similarity by applying the dot product between the transformed search query\n",
    "# and the transpose of the tf-idf matrix\n",
    "\n",
    "def search(query, model, tfidf, qty, q_type):\n",
    "    query_transform = model.transform([query])\n",
    "    similarity = np.dot(query_transform, np.transpose(tfidf))\n",
    "    x = np.array(similarity.toarray()[0])\n",
    "    print(np.argsort(x))\n",
    "    return np.argsort(x)[-qty-1:][::-1][1:] if q_type == 'id' else np.argsort(x)[-qty:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "80abc399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69134\n",
      "3\n",
      "[ 72731 113531  48880 ...  69472  69306  69133]\n",
      "[69306 69472 58032]\n",
      "[58032, 69307, 69473]\n",
      "Done! Check results.txt\n"
     ]
    }
   ],
   "source": [
    "# Run to input an article's ID and a number and get the original article and that number of related articles in a\n",
    "# 'results.txt' file, sorted asceding by date\n",
    "\n",
    "# examples: \n",
    "# 69134 for stories regarding the dispute between Apple and the US Department of Justice over providing a backdoor into a terrorist's iPhone\n",
    "# 12346 for stories related to a comedian posting an unsettling edited picture of President Trump\n",
    "# 31471 for stories relating to the Confederate flag\n",
    "# 134713 for stories relating to the 2017 London Bridge attack\n",
    "# 127683 for stories regarding a legal fight between Uber and Google\n",
    "\n",
    "o_id = int(input())\n",
    "qty = input()\n",
    "ids = search(df_processed.loc[o_id, 'content'], model, tfidf, int(qty), 'id')\n",
    "print(ids)\n",
    "locs = []\n",
    "\n",
    "\n",
    "with open('results.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('ORIGINAL ARTICLE: \\n{} - {} \\n\\n {}\\n\\n'.format(df.loc[o_id, 'title'], df.loc[o_id, 'date'], df.loc[o_id, 'content']))\n",
    "    for i in ids:\n",
    "        loc = i\n",
    "        max_sim = 0\n",
    "        for j in [i, i-1, i+1]: # checking similarities here again because of some weird indexing issue, reliable fix\n",
    "            vectorizer = TfidfVectorizer(stop_words='english', min_df=1)\n",
    "            temp = vectorizer.fit_transform([df_processed.loc[j, 'content'], df_processed.loc[o_id, 'content']])\n",
    "            sim = (np.dot(temp, temp.T).A)[0][1]\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                loc = j\n",
    "            \n",
    "            #if any([word in df_processed.loc[j, 'content'] for word in query.split()]):\n",
    "                #loc = j\n",
    "                #break\n",
    "        locs.append(loc)\n",
    "        \n",
    "    locs = sorted(locs, key=lambda id: datetime.datetime.strptime(str(df.loc[id,'date']) if str(df.loc[id,'date']) != 'NaT' else '1970-01-01 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "    print(locs)\n",
    "    \n",
    "    for loc in locs:\n",
    "        f.write('{} - {} - {} \\n\\n {}\\n\\n'.format(df.loc[loc, 'title'], df.loc[loc, 'publication'], df.loc[loc, 'date'], df.loc[loc, 'content']))\n",
    "        \n",
    "print(\"Done! Check results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73467ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[127683, 'content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f468e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ford super duty pickup\n",
      "3\n",
      "[     0  93154  93153 ... 125888  49018  48186]\n",
      "ids:  [ 48186  49018 125888]\n",
      "48186\n",
      "49018\n",
      "125888\n",
      "Done! Check results.txt\n"
     ]
    }
   ],
   "source": [
    "# Run to input a query and a number and get that number articles related to the query in a 'results.txt' file \n",
    "# sorted asceding by date\n",
    "\n",
    "# examples: \"israel embassy\", \"alabama governor\"\n",
    "\n",
    "query = input()\n",
    "qty = input()\n",
    "ids = search(query, model, tfidf, int(qty), 'query')\n",
    "print('ids: ', ids)\n",
    "locs = []\n",
    "\n",
    "with open('results.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in ids:\n",
    "        loc = i\n",
    "        max_sim = 0\n",
    "        for j in [i, i-1, i+1]: # checking similarities here again because of some weird indexing issue, reliable fix\n",
    "            vectorizer = TfidfVectorizer(stop_words='english', min_df=1)\n",
    "            temp = vectorizer.fit_transform([df_processed.loc[j, 'content'], query])\n",
    "            sim = (np.dot(temp, temp.T).A)[0][1]\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                loc = j\n",
    "        locs.append(loc)\n",
    "        \n",
    "    locs = sorted(locs, key=lambda id: datetime.datetime.strptime(str(df.loc[id,'date']) if str(df.loc[id,'date']) != 'NaT' else '1970-01-01 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "        \n",
    "    for loc in locs:\n",
    "        print(loc)\n",
    "        f.write('{} - {} - {} \\n\\n {} {}\\n\\n'.format(df.loc[loc, 'title'], df.loc[loc, 'publication'], df.loc[loc, 'date'], df.loc[loc, 'content'], loc))\n",
    "        \n",
    "print(\"Done! Check results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85b7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
